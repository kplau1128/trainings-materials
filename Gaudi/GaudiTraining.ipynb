{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaudi Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaudi System Access\n",
    "### Gaudi2 on OneCloud\n",
    "- [**Getting Started**](https://habanalabs.sharepoint.com/:w:/r/sites/GaudiSupport/Shared%20Documents/Training/1cloud-Gaudi2-reservation/1cloud%20-%20Gaudi2%20-%20Getting%20Started.docx?d=wa9b45cbd365d43679460848b8f55747d&csf=1&web=1&e=O1koUA)\n",
    "\t1. Submit AGS Request for JIRA Access of INFRA Issues\n",
    "\t\t- https://ags.intel.com/identityiq/home.jsf\n",
    "\t\t- **Search By Keywords**: `DevTools JIRA - AE DevCloud Support - User`\n",
    "\t1. Open JIRA Ticket for OneCloud INFRA issues\n",
    "\t\t- https://jira.devtools.intel.com/projects/ADSB/\n",
    "\t\t- **Component/s**: `New User Account`\n",
    "- **Gaudi2 DevCloud Reservation**\n",
    "\t1. **Login using Intel credentials**: https://reservation-gaudi-devcloud.intel.com/login\n",
    "\t1. Select: **`OneCloud`**\n",
    "\t\t1. First time login, choose **`Create A User Account for Habana Gaudi Server`**\n",
    "\t\t\t- Select the button **`Build with Parameters`** on the left tab menu\n",
    "\t\t\t- Paste your ssh public key and build only once to create the user account on Gaudi2 reservation\n",
    "\t\t\t- _If you need changes your ssh key, redo this step again_\n",
    "\t\t1. Reserve an Gaudi2 machine\n",
    "\t\t\t- Select **`Reserve Habana Gaudi Server`**\n",
    "\t\t\t- On left tab select **`Build with Parameters`**\n",
    "\t\t\t- Option of Reservation Type:\n",
    "\t\t\t\t- For single Gaudi2 card instance (1x HPU) : **`KVM (default)`**\n",
    "\t\t\t\t- For multiple Gaudi2 card instance (8x HPU): **`Baremetal`**\n",
    "\t\t\t- Optional: To choose a specific machine select **`Server_Name`** from dropdown list\n",
    "\t\t\t- By default, the time of reservation is **`120 minutes`** and you cna change based on your need.\n",
    "\t\t\t- Click **`Build`**. You will find reservation number under your name.\n",
    "\t\t\t- Click on the reservation number, and select **`Console Output`** on left tab.\n",
    "\t\t\t\t- Search for ssh login information, eg: _`ssh yourid@sysid674642-224.jf.intel.com`_ for login from terminal or connect use VSCode.\n",
    "\t\t\t- In order to change/extend the reservation time:\n",
    "\t\t\t\t- In the **`Console Output`** looks for the signature of **`Proceed or Abort`**\n",
    "\t\t\t\t- Click on **`Proceed`** will extend with your reserved time.\n",
    "- **Working Environment on Gaudi2**\n",
    "\t1. Self Starter Scripts\n",
    "\t\t- Self-starter scripts are provided as reference guidelines to get started easily with docker paths and names and setting up repos.\n",
    "\t\t- Self-starter scripts can be find in `/checkpoint/<synapse version>`, eg. 1.13.0, 1.14.0, etc. Older version can be find in `/checkpoint/archive/`\n",
    "\t1. Using [Visual Studio Code](https://code.visualstudio.com/) connect to Gaudi2 host and docker.\n",
    "\t1. Install and launch Jupyter server inside PT docker:\n",
    "\t\n",
    "\t\t```bash\n",
    "\t\t\troot@sysid674642-224:~# pip install jupyter\n",
    "\t\t\troot@sysid674642-224:~# pip install jupyter-server-proxy\n",
    "\t\t\troot@sysid674642-224:~# pip install pickleshare\n",
    "\t\t\troot@sysid674642-224:~# jupyter lab --ip sysid674642-224.jf.intel.com --port 8888 --no-browser --allow-root\n",
    "\t\t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaudi Training Resources\n",
    "- Recorded training on [DAIS CSE CWE SharePoint](https://intel.sharepoint.com/sites/dais-cse-scale/Shared%20Documents/Forms/AllItems.aspx?csf=1&web=1&e=teKCdM&cid=18ed5090%2D8210%2D4291%2Da1dd%2D5be272adad73&FolderCTID=0x012000C14D4AE12E12184582D7C0A1AA122084&id=%2Fsites%2Fdais%2Dcse%2Dscale%2FShared%20Documents%2FAI%20Application%20Enablement%2F2023%20Training%20Resource%20Repository%2F2023%20Habana%20Gaudi%20Hands%2Don%20Training%20Materials&viewid=e51be058%2D7db8%2D4c09%2Daf60%2Dc261a500f47f)\n",
    "\t- SCALE Gaudi Hands-on Training [Part 1](https://intel.sharepoint.com/:v:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/2023%20Q3%2009-22%20SCALE%20Gaudi%20Hands-on%20Training%20Part%201%20-%20Recording.mp4?csf=1&web=1&e=VMHXv2) [PPT](https://intel.sharepoint.com/:p:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/2023%20Q3%20SCALE%20Gaudi%20Hands-on%20Training%20Part%201%20-%20Presentation%20Materials.pptx?d=w1570f3fa5f5749349cd9f3e16116b911&csf=1&web=1&e=RKBaOQ)\n",
    "\t- SCALE Gaudi Hands-on Training [Part 2](https://intel.sharepoint.com/:v:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/2023%20Q3%2010-20%20SCALE%20Gaudi%20Hands-on%20Training%20Part%202%20-%20Recording.mp4?csf=1&web=1&e=pLCva4) [PPT](https://intel.sharepoint.com/:p:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/2023%20Q3%20SCALE%20Gaudi%20Hands-on%20Training%20Part%202%20-%20Presentation%20Materials.pptx?d=w1552e49b25674edfa950d9a4d5d21325&csf=1&web=1&e=3unIUy)\n",
    "\t- Additional Materials [early recorded](https://intel.sharepoint.com/:f:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/Additional%20Materials?csf=1&web=1&e=PaKCsy)\n",
    "- Habana Event and [Webinars](https://developer.habana.ai/events/)\n",
    "\t- PyTorch on Habana Gaudi [Part 1](https://developer.habana.ai/events/on-demand-pytorch-on-habana-gaudi-part-1-model-migration-distributed-training-and-profiling/): Model Migration, Distributed Training, and Profiling \n",
    "\t- PyTorch on Habana Gaudi [Part 2](https://developer.habana.ai/events/on-demand-pytorch-on-habana-gaudibr-part-2-model-migration-distributed-training-and-profiling/): Model Migration, Distributed Training, and Profiling\n",
    "\t- Habana Gaudi [Training notes](https://intel-my.sharepoint.com/:w:/r/personal/yashesh_a_shroff_intel_com/_layouts/15/doc2.aspx?sourcedoc=%7BE3ECD187-00A9-428B-B5F8-602B63251F0C%7D&file=SCALE%20Habana%20Training.docx&action=default&mobileredirect=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habana Documents\n",
    "- Habana Gaudi [Documentation](https://docs.habana.ai/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habana Repositories:\n",
    "-  On [HabanaAI](https://github.com/HabanaAI)\n",
    "\t- [Model References](https://github.com/HabanaAI/Model-References)\n",
    "\t- [Gaudi2 Workshop](https://github.com/HabanaAI/Gaudi2-Workshop)\n",
    "\t- [Gaudi Tutorials](https://github.com/HabanaAI/Gaudi-tutorials)\n",
    "\t- Others\n",
    "\t\t- [Habana DeepSpeed forked](https://github.com/HabanaAI/DeepSpeed)\n",
    "\t\t- [Gaudi Solutions](https://github.com/HabanaAI/Gaudi-solutions)\n",
    "- Habana on [Huggingface](https://huggingface.co/Habana)\n",
    "\t- [Optimum-Habana](https://github.com/huggingface/optimum-habana)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run from host\n",
    "# %cd ~/gaudi2_starter\n",
    "\n",
    "# Run from docker (~/, /root)\n",
    "%cd ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Model References repository\n",
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Gaudi2 workshop repository\n",
    "!git clone https://github.com/HabanaAI/Gaudi2-Workshop.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Gaudi tutorials repository\n",
    "!git clone https://github.com/HabanaAI/Gaudi-tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Huggingface Optimum Habana repository\n",
    "!git clone https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone DeepSpeed (Habana folked) repository\n",
    "#!git clone https://github.com/HabanaAI/DeepSpeed.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Gaudi solution repository\n",
    "#!git clone https://github.com/HabanaAI/Gaudi-solutions.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "> _Run following examples inside docker_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples from [SCALE Gaudi Hands-on Training Part 1](https://intel.sharepoint.com/:v:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/2023%20Q3%2009-22%20SCALE%20Gaudi%20Hands-on%20Training%20Part%201%20-%20Recording.mp4?csf=1&web=1&e=t1fijw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST - Manual Migration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download manual migration version of mnist code\n",
    "!curl -O https://raw.githubusercontent.com/leopck/Habana-Workshop/main/mnist-manual-migration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056375252 KB\n",
      "------------------------------------------------------------------------------\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311083\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.135186\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.626469\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.435229\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.426359\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.341576\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.184721\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.175060\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.356484\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.183976\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.238811\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.265356\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.243382\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.183390\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.282184\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.105883\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.313036\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.134687\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.426026\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.175658\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.132405\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.180363\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.100194\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.393948\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.105683\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.172729\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.182322\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.049018\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.077729\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.112241\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.321200\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.125802\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.028265\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.114580\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.015297\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.108324\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.214612\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.106355\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.027609\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.023613\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.140873\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.126642\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.332537\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.126639\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.167441\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.108816\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.045932\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.075814\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.094396\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.141224\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162770\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.231025\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.148452\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.019610\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.017527\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.164024\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.125139\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.050669\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.145827\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.113770\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.110372\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.031610\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.047522\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.121482\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.202443\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.080589\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.021480\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.163523\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.161241\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.035676\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.156240\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.148790\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.253149\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.240621\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.136368\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.080538\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.028044\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.005406\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.119940\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.034311\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.113140\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.026065\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.007033\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.179772\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.042769\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.017638\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.038630\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.155967\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.025217\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.068649\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.159077\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.145649\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.018044\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.001457\n",
      "\n",
      "Test set: Average loss: 0.0428, Accuracy: 9857/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 python ./mnist-manual-migration.py --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST - GPU Migration Toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-38-18/gpu_migration_27593.log\n",
      "[2024-03-19 02:38:18] /root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/main.py:106\n",
      "    [context]:     use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "use_cuda: True\n",
      "[2024-03-19 02:38:19] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=1, ) --> torch.hpu.random.manual_seed_all(1)\n",
      "\u001b[0m\n",
      "[2024-03-19 02:38:19] /root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/main.py:136\n",
      "    [context]:     train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.utils.data.DataLoader.__init__(dataset=dataset, batch_size=64, shuffle=True, sampler=None, batch_sampler=None, num_workers=1, collate_fn=None, pin_memory=True, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, prefetch_factor=None, persistent_workers=False, pin_memory_device=, ) --> change pin_memory_device to hpu\n",
      "\u001b[0m\n",
      "[2024-03-19 02:38:19] /root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/main.py:137\n",
      "    [context]:     test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.utils.data.DataLoader.__init__(dataset=dataset, batch_size=1000, shuffle=True, sampler=None, batch_sampler=None, num_workers=1, collate_fn=None, pin_memory=True, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, prefetch_factor=None, persistent_workers=False, pin_memory_device=, ) --> change pin_memory_device to hpu\n",
      "\u001b[0m\n",
      "[2024-03-19 02:38:19] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:173\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056375252 KB\n",
      "------------------------------------------------------------------------------\n",
      "[2024-03-19 02:38:21] /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1054\n",
      "    [context]:                 current_device = torch.cuda.current_device()  # choose cuda for default\n",
      "\n",
      "\u001b[93m    [hpu_modified]: torch.cuda.current_device() --> habana_frameworks.torch.gpu_migration.torch.cuda.current_device()\n",
      "\u001b[0m\n",
      "[2024-03-19 02:38:21] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=3961684187514624903, ) --> torch.hpu.random.manual_seed_all(3961684187514624903)\n",
      "\u001b[0m\n",
      "[2024-03-19 02:38:21] /root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/main.py:45\n",
      "    [context]:         data, target = data.to(device), target.to(device)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={}, ) --> torch.Tensor.to(args=('hpu',), kwargs={})\n",
      "\u001b[0m\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303176\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.691506\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.064393\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.624223\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.360884\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.482517\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.298114\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.606262\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.224434\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.260378\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.190481\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.318582\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.267331\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.201416\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.383809\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.127367\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.158996\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.136857\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.235409\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.100604\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.094185\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.181506\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.296144\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.111214\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.250152\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.300414\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.077754\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.227343\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.190664\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.295142\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.121703\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.140916\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.243351\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.121081\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.301585\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.111476\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.279409\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.181143\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.058942\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.122759\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.170895\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.149966\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.232570\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.101145\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.230803\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.037653\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.158002\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.168151\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.099117\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.114746\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.245497\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.057648\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.094716\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.105507\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.154734\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.052328\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.114998\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.272278\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.068247\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.064031\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091921\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.042337\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.354613\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.109826\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.077247\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.200654\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.038148\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.095488\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.044131\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.104255\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.080501\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.217649\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.059065\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.128992\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.129292\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.075683\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.208196\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.077228\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.175798\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.061717\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.166175\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.111363\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.133335\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.023808\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.094948\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.173827\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.118921\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.080786\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.125019\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.128927\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.013661\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.134002\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.083279\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.102800\n",
      "[2024-03-19 02:38:31] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=897815624723325961, ) --> torch.hpu.random.manual_seed_all(897815624723325961)\n",
      "\u001b[0m\n",
      "[2024-03-19 02:38:31] /root/Model-References/PyTorch/examples/gpu_migration/simple_examples/mnist/main.py:67\n",
      "    [context]:             data, target = data.to(device), target.to(device)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={}, ) --> torch.Tensor.to(args=('hpu',), kwargs={})\n",
      "\u001b[0m\n",
      "\n",
      "Test set: Average loss: 0.0502, Accuracy: 9832/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 python main.py --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet - GPU Migration Toolkit\n",
    "\n",
    "Also can find a notebook for this example in [Gaudi2-Workshop/Model-Migration](https://github.com/HabanaAI/Gaudi2-Workshop/blob/main/Model-Migration/model_migrate.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-19 02:06:39,621] torch.distributed.run: [WARNING] \n",
      "[2024-03-19 02:06:39,621] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-03-19 02:06:39,621] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-03-19 02:06:39,621] torch.distributed.run: [WARNING] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1636.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1639.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1633.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1640.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1637.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1634.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1638.log\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-19/02-06-42/gpu_migration_1635.log\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=3, ) --> torch.hpu.set_device(hpu:3)\n",
      "\u001b[0m\n",
      "| distributed init (rank 3): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=3, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=6, ) --> torch.hpu.set_device(hpu:6)\n",
      "\u001b[0m\n",
      "| distributed init (rank 6): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=6, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=0, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=7, ) --> torch.hpu.set_device(hpu:7)\n",
      "\u001b[0m\n",
      "| distributed init (rank 7): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=7, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=4, ) --> torch.hpu.set_device(hpu:4)\n",
      "\u001b[0m\n",
      "| distributed init (rank 4): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=4, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=1, ) --> torch.hpu.set_device(hpu:1)\n",
      "\u001b[0m\n",
      "| distributed init (rank 1): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=1, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=5, ) --> torch.hpu.set_device(hpu:5)\n",
      "\u001b[0m\n",
      "| distributed init (rank 5): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=5, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=2, ) --> torch.hpu.set_device(hpu:2)\n",
      "\u001b[0m\n",
      "| distributed init (rank 2): env://\n",
      "[2024-03-19 02:06:42] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=2, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "Namespace(use_torch_compile=False, data_path='/data/pytorch/imagenet/ILSVRC2012', model='resnet50', device='cuda', batch_size=256, epochs=1, dl_worker_type='HABANA', workers=8, opt='sgd', lr=0.1, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='custom_lr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=10, output_dir='.', resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=True, world_size=8, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, save_checkpoint=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
      "[2024-03-19 02:06:43] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Took 389.73338556289673\n",
      "Loading validation data\n",
      "Creating data loaders\n",
      "HabanaDataLoader device type  4\n",
      "Warning: Updated shuffle to True as sampler is DistributedSampler with shuffle True\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f6ea1172500>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform RandomResizedCrop: Random Crop,Resize w:h  224 224  scale:  (0.08, 1.0)  ratio:  (0.75, 1.3333333333333333)  interpolation:  InterpolationMode.BILINEAR\n",
      "transform RandomHorizontalFlip: probability  0.5\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:1\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056375252 KB\n",
      "------------------------------------------------------------------------------\n",
      "MediaDataloader 0/8 seed : 1411578726\n",
      "Decode ResizedCrop w:h 224 224\n",
      "MediaDataloader shuffle is  True\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... MediaDataloader 1/8 seed : 1627495689\n",
      "MediaDataloader 4/8 seed : 1638917786\n",
      "MediaDataloader 5/8 seed : 1650391116\n",
      "MediaDataloader 7/8 seed : 1672378502\n",
      "MediaDataloader 6/8 seed : 1687695470\n",
      "MediaDataloader 3/8 seed : 1690942465\n",
      "MediaDataloader 2/8 seed : 1693545463\n",
      "Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 1281167 classes 1000\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  1411578726\n",
      "sliced media files/labels 160146\n",
      "Finding largest file ...\n",
      "MediaDataloader 3/8 seed : 1067276158MediaDataloader 7/8 seed : 1067280774\n",
      "\n",
      "MediaDataloader 6/8 seed : 1067891457\n",
      "MediaDataloader 5/8 seed : 1674432292\n",
      "MediaDataloader 4/8 seed : 393958703\n",
      "MediaDataloader 1/8 seed : 420977436\n",
      "MediaDataloader 2/8 seed : 477725590\n",
      "largest file is  /data/pytorch/imagenet/ILSVRC2012/train/n02490219/n02490219_11648.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "HabanaDataLoader device type  4\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f6ea118da80>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform Resize: w:h  256 256  interpolation:  InterpolationMode.BILINEAR  max_size:  None\n",
      "transform CenterCrop: w:h  224 224\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:2\n",
      "MediaDataloader 0/8 seed : 713076747\n",
      "Decode w:h  256 256  , Crop disabled\n",
      "MediaDataloader shuffle is  False\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 50000 classes 1000\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  713076747\n",
      "sliced media files/labels 6250\n",
      "Finding largest file ...\n",
      "largest file is  /data/pytorch/imagenet/ILSVRC2012/val/n01630670/ILSVRC2012_val_00046430.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "Creating model\n",
      "[2024-03-19 02:17:03] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:173\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "Using HPU Graphs on Gaudi2 for reducing operator accumulation time.\n",
      "[2024-03-19 02:17:03] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:316\n",
      "    [context]:     scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.__init__(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True, ) --> set enabled to Flase\n",
      "\u001b[0m\n",
      "[2024-03-19 02:17:03] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:359\n",
      "    [context]:         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], broadcast_buffers=False, gradient_as_bucket_view=True, bucket_cap_mb=1024)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.nn.parallel.DistributedDataParallel.__init__(module=module, device_ids=[0], output_device=None, dim=0, broadcast_buffers=False, process_group=None, bucket_cap_mb=1024, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=True, static_graph=False, delay_all_reduce_named_params=None, param_to_hook_all_reduce=None, mixed_precision=None, ) --> change device_ids and output_device to None\n",
      "\u001b[0m\n",
      "Start training\n",
      "[2024-03-19 02:17:04] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:113\n",
      "    [context]:         if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Shuffling ... Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Done!\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2024-03-19 02:17:05] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:33\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-03-19 02:17:05] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:34\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2024-03-19 02:17:05] /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/common.py:9\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec(\"torch_xla\"))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-03-19 02:17:17] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-03-19 02:17:17] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/626]  eta: 2:10:30  lr: 0.1  img/s: 20.465661218003827  loss: 7.1485 (7.1485)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.7812)  time: 12.5088  data: 1.1523  max mem: 18669\n",
      "Epoch: [0]  [ 10/626]  eta: 0:13:14  lr: 0.1  img/s: 1514.0768608375452  loss: 7.0855 (7.1170)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9766)  time: 1.2903  data: 0.1265  max mem: 61936\n",
      "Epoch: [0]  [ 20/626]  eta: 0:07:24  lr: 0.1  img/s: 2053.9327617155877  loss: 7.1485 (7.1667)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (1.0417)  time: 0.1455  data: 0.0458  max mem: 61936\n",
      "Epoch: [0]  [ 30/626]  eta: 0:05:22  lr: 0.1  img/s: 1873.2604260930111  loss: 7.0855 (7.1260)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8789)  time: 0.1287  data: 0.0721  max mem: 61936\n",
      "Epoch: [0]  [ 40/626]  eta: 0:04:21  lr: 0.1  img/s: 1663.7378830926066  loss: 7.0965 (7.1201)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.7812)  time: 0.1433  data: 0.0829  max mem: 61936\n",
      "Epoch: [0]  [ 50/626]  eta: 0:03:41  lr: 0.1  img/s: 1840.445446896124  loss: 7.0855 (7.0814)  acc1: 0.3906 (0.3255)  acc5: 0.3906 (0.6510)  time: 0.1444  data: 0.0864  max mem: 61936\n",
      "Epoch: [0]  [ 60/626]  eta: 0:03:15  lr: 0.1  img/s: 1829.6409786809206  loss: 7.0855 (7.0579)  acc1: 0.3906 (0.2790)  acc5: 0.7812 (0.6696)  time: 0.1374  data: 0.0872  max mem: 61936\n",
      "Epoch: [0]  [ 70/626]  eta: 0:02:56  lr: 0.1  img/s: 1742.551598583163  loss: 7.0040 (7.0392)  acc1: 0.3906 (0.2441)  acc5: 0.7812 (0.7324)  time: 0.1414  data: 0.0901  max mem: 61936\n",
      "Epoch: [0]  [ 80/626]  eta: 0:02:41  lr: 0.1  img/s: 1708.196628570874  loss: 7.0040 (7.0175)  acc1: 0.3906 (0.2170)  acc5: 0.7812 (0.7812)  time: 0.1463  data: 0.0876  max mem: 61936\n",
      "Epoch: [0]  [ 90/626]  eta: 0:02:28  lr: 0.1  img/s: 1901.6218216993925  loss: 6.9171 (6.9958)  acc1: 0.0000 (0.1953)  acc5: 0.7812 (0.8594)  time: 0.1402  data: 0.0828  max mem: 61936\n",
      "Epoch: [0]  [100/626]  eta: 0:02:19  lr: 0.1  img/s: 1658.3858573939474  loss: 6.9171 (6.9597)  acc1: 0.3906 (0.3196)  acc5: 1.1719 (1.0653)  time: 0.1424  data: 0.0889  max mem: 61936\n",
      "Epoch: [0]  [110/626]  eta: 0:02:11  lr: 0.1  img/s: 1789.0139196756209  loss: 6.9080 (6.9392)  acc1: 0.3906 (0.3255)  acc5: 1.1719 (1.0742)  time: 0.1467  data: 0.0887  max mem: 61936\n",
      "Epoch: [0]  [120/626]  eta: 0:02:03  lr: 0.1  img/s: 1831.936599070606  loss: 6.9080 (6.9102)  acc1: 0.3906 (0.3305)  acc5: 1.1719 (1.2921)  time: 0.1394  data: 0.0823  max mem: 61936\n",
      "Epoch: [0]  [130/626]  eta: 0:01:57  lr: 0.1  img/s: 1826.1579996278776  loss: 6.8878 (6.8841)  acc1: 0.3906 (0.3069)  acc5: 1.1719 (1.3672)  time: 0.1379  data: 0.0911  max mem: 61936\n",
      "Epoch: [0]  [140/626]  eta: 0:01:51  lr: 0.1  img/s: 1811.5095020828933  loss: 6.8878 (6.8577)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (1.6146)  time: 0.1387  data: 0.0980  max mem: 61936\n",
      "Epoch: [0]  [150/626]  eta: 0:01:46  lr: 0.1  img/s: 1816.4854802705759  loss: 6.8438 (6.8349)  acc1: 0.3906 (0.4883)  acc5: 1.1719 (1.7578)  time: 0.1391  data: 0.0992  max mem: 61936\n",
      "Epoch: [0]  [160/626]  eta: 0:01:42  lr: 0.1  img/s: 1681.9066567983616  loss: 6.8438 (6.8157)  acc1: 0.3906 (0.5515)  acc5: 1.1719 (1.8612)  time: 0.1446  data: 0.1046  max mem: 61936\n",
      "Epoch: [0]  [170/626]  eta: 0:01:37  lr: 0.1  img/s: 1771.094826169946  loss: 6.8007 (6.7919)  acc1: 0.3906 (0.5859)  acc5: 1.1719 (2.0182)  time: 0.1466  data: 0.1077  max mem: 61936\n",
      "Epoch: [0]  [180/626]  eta: 0:01:33  lr: 0.1  img/s: 1795.434793659287  loss: 6.8007 (6.7656)  acc1: 0.3906 (0.6373)  acc5: 1.1719 (2.2615)  time: 0.1419  data: 0.1053  max mem: 61936\n",
      "Epoch: [0]  [190/626]  eta: 0:01:30  lr: 0.1  img/s: 1765.7944614396888  loss: 6.7139 (6.7435)  acc1: 0.3906 (0.6641)  acc5: 1.1719 (2.3633)  time: 0.1419  data: 0.1102  max mem: 61936\n",
      "Epoch: [0]  [200/626]  eta: 0:01:26  lr: 0.1  img/s: 1744.6139137979499  loss: 6.5985 (6.7263)  acc1: 0.3906 (0.7068)  acc5: 1.5625 (2.4740)  time: 0.1440  data: 0.1137  max mem: 61936\n",
      "Epoch: [0]  [210/626]  eta: 0:01:23  lr: 0.1  img/s: 1797.1188037214995  loss: 6.5617 (6.7030)  acc1: 0.3906 (0.7280)  acc5: 2.3438 (2.5568)  time: 0.1428  data: 0.1072  max mem: 61936\n",
      "Epoch: [0]  [220/626]  eta: 0:01:20  lr: 0.1  img/s: 1644.8742326095833  loss: 6.5450 (6.6799)  acc1: 0.3906 (0.7303)  acc5: 3.1250 (2.7004)  time: 0.1472  data: 0.1053  max mem: 61936\n",
      "Epoch: [0]  [230/626]  eta: 0:01:17  lr: 0.1  img/s: 1959.8141926854705  loss: 6.5083 (6.6575)  acc1: 0.7812 (0.8138)  acc5: 3.5156 (2.8809)  time: 0.1414  data: 0.1008  max mem: 61936\n",
      "Epoch: [0]  [240/626]  eta: 0:01:14  lr: 0.1  img/s: 1771.2023384172794  loss: 6.4924 (6.6357)  acc1: 1.1719 (0.8281)  acc5: 3.9062 (2.9688)  time: 0.1360  data: 0.1002  max mem: 61936\n",
      "Epoch: [0]  [250/626]  eta: 0:01:11  lr: 0.1  img/s: 1785.017317501266  loss: 6.4883 (6.6113)  acc1: 1.1719 (0.8864)  acc5: 3.9062 (3.0649)  time: 0.1424  data: 0.1054  max mem: 61936\n",
      "Epoch: [0]  [260/626]  eta: 0:01:09  lr: 0.1  img/s: 1894.5772841674066  loss: 6.3877 (6.5844)  acc1: 1.1719 (0.9259)  acc5: 4.2969 (3.1973)  time: 0.1376  data: 0.1013  max mem: 61936\n",
      "Epoch: [0]  [270/626]  eta: 0:01:06  lr: 0.1  img/s: 1795.066799562527  loss: 6.3809 (6.5611)  acc1: 1.1719 (0.9487)  acc5: 4.2969 (3.3482)  time: 0.1371  data: 0.1016  max mem: 61936\n",
      "Epoch: [0]  [280/626]  eta: 0:01:04  lr: 0.1  img/s: 1753.4483778417307  loss: 6.3247 (6.5448)  acc1: 1.5625 (0.9833)  acc5: 4.6875 (3.5156)  time: 0.1423  data: 0.1091  max mem: 61936\n",
      "Epoch: [0]  [290/626]  eta: 0:01:01  lr: 0.1  img/s: 1790.7644450122307  loss: 6.2917 (6.5155)  acc1: 1.5625 (1.0677)  acc5: 4.6875 (3.7760)  time: 0.1426  data: 0.1124  max mem: 61936\n",
      "Epoch: [0]  [300/626]  eta: 0:00:59  lr: 0.1  img/s: 1724.745990969089  loss: 6.2135 (6.4982)  acc1: 1.5625 (1.1215)  acc5: 5.0781 (3.9693)  time: 0.1438  data: 0.1114  max mem: 61936\n",
      "Epoch: [0]  [310/626]  eta: 0:00:57  lr: 0.1  img/s: 1855.118462417735  loss: 6.1719 (6.4774)  acc1: 1.5625 (1.1719)  acc5: 5.0781 (4.0894)  time: 0.1414  data: 0.1107  max mem: 61936\n",
      "Epoch: [0]  [320/626]  eta: 0:00:55  lr: 0.1  img/s: 1774.30828963145  loss: 6.1426 (6.4556)  acc1: 1.5625 (1.2074)  acc5: 5.4688 (4.3679)  time: 0.1393  data: 0.1133  max mem: 61936\n",
      "Epoch: [0]  [330/626]  eta: 0:00:53  lr: 0.1  img/s: 1727.4725392117132  loss: 6.1134 (6.4325)  acc1: 1.5625 (1.2408)  acc5: 5.8594 (4.5726)  time: 0.1444  data: 0.1149  max mem: 61936\n",
      "Epoch: [0]  [340/626]  eta: 0:00:51  lr: 0.1  img/s: 1718.1779797913528  loss: 6.0877 (6.4161)  acc1: 1.9531 (1.3058)  acc5: 6.6406 (4.6987)  time: 0.1468  data: 0.1114  max mem: 61936\n",
      "Epoch: [0]  [350/626]  eta: 0:00:48  lr: 0.1  img/s: 1731.0238824271567  loss: 6.0018 (6.3910)  acc1: 1.9531 (1.4214)  acc5: 6.6406 (5.0347)  time: 0.1466  data: 0.1083  max mem: 61936\n",
      "Epoch: [0]  [360/626]  eta: 0:00:46  lr: 0.1  img/s: 1805.7753364611815  loss: 5.9788 (6.3690)  acc1: 1.9531 (1.4675)  acc5: 7.0312 (5.3421)  time: 0.1431  data: 0.0983  max mem: 61936\n",
      "Epoch: [0]  [370/626]  eta: 0:00:44  lr: 0.1  img/s: 1765.8214680464876  loss: 5.9330 (6.3510)  acc1: 1.9531 (1.4597)  acc5: 7.4219 (5.4379)  time: 0.1415  data: 0.0885  max mem: 61936\n",
      "Epoch: [0]  [380/626]  eta: 0:00:42  lr: 0.1  img/s: 1822.1092371078107  loss: 5.8839 (6.3345)  acc1: 2.3438 (1.5325)  acc5: 7.8125 (5.6490)  time: 0.1407  data: 0.0881  max mem: 61936\n",
      "Epoch: [0]  [390/626]  eta: 0:00:41  lr: 0.1  img/s: 1764.7473585701387  loss: 5.8583 (6.3175)  acc1: 2.3438 (1.5918)  acc5: 8.2031 (5.8496)  time: 0.1408  data: 0.0929  max mem: 61936\n",
      "Epoch: [0]  [400/626]  eta: 0:00:39  lr: 0.1  img/s: 1708.98807081432  loss: 5.8328 (6.2962)  acc1: 2.3438 (1.6959)  acc5: 8.9844 (6.0880)  time: 0.1456  data: 0.0947  max mem: 61936\n",
      "Epoch: [0]  [410/626]  eta: 0:00:37  lr: 0.1  img/s: 1814.178432375699  loss: 5.7593 (6.2801)  acc1: 2.7344 (1.7578)  acc5: 8.9844 (6.2686)  time: 0.1437  data: 0.0884  max mem: 61936\n",
      "Epoch: [0]  [420/626]  eta: 0:00:35  lr: 0.1  img/s: 1753.298633404549  loss: 5.7085 (6.2594)  acc1: 2.7344 (1.7805)  acc5: 9.7656 (6.4499)  time: 0.1416  data: 0.0868  max mem: 61936\n",
      "Epoch: [0]  [430/626]  eta: 0:00:33  lr: 0.1  img/s: 1823.062091887783  loss: 5.6858 (6.2398)  acc1: 2.7344 (1.8732)  acc5: 11.3281 (6.6850)  time: 0.1412  data: 0.0886  max mem: 61936\n",
      "Epoch: [0]  [440/626]  eta: 0:00:31  lr: 0.1  img/s: 1868.478350109308  loss: 5.6696 (6.2201)  acc1: 2.7344 (1.9618)  acc5: 11.3281 (6.8750)  time: 0.1367  data: 0.0857  max mem: 61936\n",
      "Epoch: [0]  [450/626]  eta: 0:00:29  lr: 0.1  img/s: 1815.973968163768  loss: 5.6649 (6.1998)  acc1: 3.1250 (2.1145)  acc5: 13.2812 (7.1841)  time: 0.1371  data: 0.0857  max mem: 61936\n",
      "Epoch: [0]  [460/626]  eta: 0:00:28  lr: 0.1  img/s: 1832.686090527787  loss: 5.6553 (6.1830)  acc1: 3.5156 (2.1858)  acc5: 13.6719 (7.3637)  time: 0.1386  data: 0.0896  max mem: 61936\n",
      "Epoch: [0]  [470/626]  eta: 0:00:26  lr: 0.1  img/s: 1806.6564612584284  loss: 5.6216 (6.1705)  acc1: 3.5156 (2.2054)  acc5: 13.6719 (7.5033)  time: 0.1389  data: 0.0880  max mem: 61936\n",
      "Epoch: [0]  [480/626]  eta: 0:00:24  lr: 0.1  img/s: 1782.760749112224  loss: 5.5840 (6.1542)  acc1: 3.5156 (2.2800)  acc5: 13.6719 (7.7089)  time: 0.1408  data: 0.0848  max mem: 61936\n",
      "Epoch: [0]  [490/626]  eta: 0:00:22  lr: 0.1  img/s: 1802.2458025186818  loss: 5.5776 (6.1368)  acc1: 3.9062 (2.3750)  acc5: 14.0625 (8.0000)  time: 0.1409  data: 0.0859  max mem: 61936\n",
      "Epoch: [0]  [500/626]  eta: 0:00:20  lr: 0.1  img/s: 1762.0395999569394  loss: 5.5113 (6.1189)  acc1: 4.2969 (2.5199)  acc5: 14.0625 (8.2644)  time: 0.1416  data: 0.0897  max mem: 61936\n",
      "Epoch: [0]  [510/626]  eta: 0:00:19  lr: 0.1  img/s: 1832.283909156933  loss: 5.4411 (6.1041)  acc1: 4.2969 (2.5541)  acc5: 15.2344 (8.5261)  time: 0.1405  data: 0.0906  max mem: 61936\n",
      "Epoch: [0]  [520/626]  eta: 0:00:17  lr: 0.1  img/s: 1790.5139043322179  loss: 5.4124 (6.0848)  acc1: 4.2969 (2.6091)  acc5: 15.6250 (8.7338)  time: 0.1393  data: 0.0916  max mem: 61936\n",
      "Epoch: [0]  [530/626]  eta: 0:00:15  lr: 0.1  img/s: 1774.4877438358908  loss: 5.3954 (6.0681)  acc1: 5.4688 (2.7054)  acc5: 15.6250 (8.9699)  time: 0.1416  data: 0.0993  max mem: 61936\n",
      "Epoch: [0]  [540/626]  eta: 0:00:14  lr: 0.1  img/s: 1761.2190697641613  loss: 5.3901 (6.0523)  acc1: 5.4688 (2.7841)  acc5: 16.4062 (9.1761)  time: 0.1428  data: 0.1046  max mem: 61936\n",
      "Epoch: [0]  [550/626]  eta: 0:00:12  lr: 0.1  img/s: 1765.5438911387355  loss: 5.3723 (6.0362)  acc1: 5.4688 (2.8948)  acc5: 16.4062 (9.4448)  time: 0.1431  data: 0.1035  max mem: 61936\n",
      "Epoch: [0]  [560/626]  eta: 0:00:10  lr: 0.1  img/s: 1759.075257539589  loss: 5.3556 (6.0157)  acc1: 5.8594 (3.0496)  acc5: 16.7969 (9.7382)  time: 0.1432  data: 0.0998  max mem: 61936\n",
      "Epoch: [0]  [570/626]  eta: 0:00:09  lr: 0.1  img/s: 1701.7589450995308  loss: 5.3502 (6.0013)  acc1: 5.8594 (3.1048)  acc5: 17.5781 (9.9205)  time: 0.1459  data: 0.0968  max mem: 61936\n",
      "Epoch: [0]  [580/626]  eta: 0:00:07  lr: 0.1  img/s: 1807.9605830441556  loss: 5.2834 (5.9882)  acc1: 5.8594 (3.1780)  acc5: 19.5312 (10.0834)  time: 0.1440  data: 0.0917  max mem: 61936\n",
      "Epoch: [0]  [590/626]  eta: 0:00:05  lr: 0.1  img/s: 1809.7761591125177  loss: 5.2832 (5.9679)  acc1: 5.8594 (3.3464)  acc5: 19.5312 (10.3320)  time: 0.1395  data: 0.0919  max mem: 61936\n",
      "Epoch: [0]  [600/626]  eta: 0:00:04  lr: 0.1  img/s: 1768.0560012435353  loss: 5.2287 (5.9531)  acc1: 6.2500 (3.3940)  acc5: 20.3125 (10.5405)  time: 0.1411  data: 0.0971  max mem: 61936\n",
      "Epoch: [0]  [610/626]  eta: 0:00:02  lr: 0.1  img/s: 1755.1965230252479  loss: 5.2241 (5.9383)  acc1: 6.2500 (3.5093)  acc5: 20.3125 (10.7989)  time: 0.1433  data: 0.0957  max mem: 61936\n",
      "Epoch: [0]  [620/626]  eta: 0:00:00  lr: 0.1  img/s: 1796.1126456827815  loss: 5.1999 (5.9204)  acc1: 7.0312 (3.5838)  acc5: 21.0938 (11.0491)  time: 0.1422  data: 0.0966  max mem: 61936\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Epoch: [0] Total time: 0:01:42\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2024-03-19 02:18:46] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:83\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-03-19 02:18:46] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:84\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:30  loss: 4.8531 (4.8531)  acc1: 9.7656 (9.7656)  acc5: 28.5156 (28.5156)  time: 6.0236  data: 0.1540  max mem: 61936\n",
      "Test:  Total time: 0:00:09\n",
      "[2024-03-19 02:18:56] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-03-19 02:18:56] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 123.07332372665405],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 123.07332372665405],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-03-19 02:18:56] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 61100.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 61100.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-03-19 02:18:56] /root/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 160100.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 160100.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 9.463 Acc@5 24.684\n",
      "Training time 0:01:51\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 8 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples from [SCALE Gaudi Hands-on Training Part 2](https://intel.sharepoint.com/:v:/r/sites/dais-cse-scale/Shared%20Documents/AI%20Application%20Enablement/2023%20Training%20Resource%20Repository/2023%20Habana%20Gaudi%20Hands-on%20Training%20Materials/2023%20Q3%2010-20%20SCALE%20Gaudi%20Hands-on%20Training%20Part%202%20-%20Recording.mp4?csf=1&web=1&e=VxdTBW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Dynamic Shapes\n",
    "\n",
    "Notebook from [Gaudi-tutorials/PyTorch/Detecting_Dynamic_Shapes](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/PyTorch/Detecting_Dynamic_Shapes/Detecting_Dynamic_Shapes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Profiling and Optimization\n",
    "\n",
    "Notebook from [Gaudi-tutorials/PyTorch/Profiling_and_Optimization](https://github.com/HabanaAI/Gaudi-tutorials/tree/main/PyTorch/Profiling_and_Optimization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
